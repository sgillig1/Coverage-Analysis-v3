{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9338d2d-9fa4-4403-9362-eb74675a975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script Name: ChainLAMP_coverage_analysis_v3.4.ipynb\n",
    "Version: v3.4\n",
    "Author: Shane Gilligan-Steinberg\n",
    "Date: 240822\n",
    "\n",
    "Description: ******** START HERE PLEASE ********\n",
    "Each of the cells of this jupyter notebook can operated separately.\n",
    "\n",
    "Process:\n",
    "[1] Coverage Software - For initial testing you can skip extension and go straight here as there are test files already set up. Make sure to run [1A]-[1B]-[1C] in order\n",
    "    [1A] Setup for all neccesary inputs - please go through the folder files to understand how assays and primers are denoted.\n",
    "    [1B] Running the actual code to determine coverage\n",
    "\n",
    "Additional software: Script Name: ChainLAMP_coverage_analysis_v3.4_extension.ipynb\n",
    "This can be used to generate a sets of input sequences organized by subtype for use in pipeline\n",
    "[1] Generate library of sequences to be inputted into the pipeline (from LANL alignment - need to remove gaps and organize by subtype).\n",
    "Other capability of splitting by year is not available in this version\n",
    "    [1B] Generate alignments with additional split by year of sequeneces\n",
    "    [1C] Another option is to gather sequences from GenBank IDs\n",
    "\"\"\"\n",
    "# Install all neccesary libraries\n",
    "!pip install biopython pandas matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f66bcc-ba51-4eb2-a25a-8e8fd9d819c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'output_name_base': 'Validation', 'assays': 'Targets/240710_Test.csv', 'alignments': 'Alignments/Subtypes_240710_Test.csv', 'rosalind_threshold': 3, 'testing': True}]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script Name: ChainLAMP_coverage_analysis_v2.7.ipynb [1A]\n",
    "Version: v2.11\n",
    "Author: Shane Gilligan-Steinberg\n",
    "Date: 240715\n",
    "\n",
    "Description:\n",
    "Run analysis of primer sets agains libraries of HIV sequences.\n",
    "Aligns primers within assays to libraries of sequences (organizes by subtypes or other metric). Finds best alignment (no gaps) and all mismatches.\n",
    "Then identifies coverage in a few ways (1) Perfect coverage (2) Coverage with single mismatch \n",
    "(3) Coverage with single mismatch (not in last 3 bp of critical termini) (4) Coverage with single mismatch (not in last 3 bp of critical termini)\n",
    "(5) Coverage based on ROSALIND Scoring\n",
    "Could add extra readout of coverage by assay and primer (for development)\n",
    "\n",
    "Input:\n",
    "1. List of assays (.csv) [~/Assays]\n",
    "2. List of primer sequences for each assay (.fasta) [~/Assays]\n",
    "- This software enables assessment of original and reverse complement.\n",
    "- Report primers as 5'-3' with 3' end as critical termini (e.x. Report RC of F1 as the 5' is the critical termini)\n",
    "- For LAMP, use primer regions. Make sure to be careful about where you split F1/F2 and B1/B2\n",
    "- Probes: add NNNNN if direction of alignment is not important\n",
    "3. .csv list of subtypes (can use script 230704_Split_FASTA_v2_CRF.py to perform separation from LANL alignmnents) [~/Targets]\n",
    "4. .fasta library of HIV sequences separated by subtype (can use script 230704_Split_FASTA_v2_CRF.py to perform separation\n",
    "from LANL alignmnents) [~/Alignments]\n",
    "\n",
    "Outputs: In folder (\"/Outputs\")\n",
    "1. NAME_organized_data.json: alignment for all primers\n",
    "2. NAME_aggregate.json: alignmeng by assay\n",
    "3. NAME_coverage.json: coverage by subtype\n",
    "4. NAME_coverage.csv: coverage\n",
    "5. NAME_organized_data_primer.json: coverage\n",
    "6. NAME_by_base.json: coverage at the base pair level\n",
    "7. NAME_case_info.json: information about the software run\n",
    "\"\"\"\n",
    "\n",
    "# Import libraries\n",
    "import time\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils import nt_search\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from Bio import Align\n",
    "from Bio.Seq import Seq\n",
    "import json\n",
    "import tqdm\n",
    "import csv\n",
    "\n",
    "# Parameters\n",
    "testing = False # True disables the global coverage weighting\n",
    "globalAnalysis = False # True enables different coverage by year or test case\n",
    "singleBaseAnalysis = True # True enables a readout of the coverage at the base pair level\n",
    "enableGaps = False # The code is not able to parse gaps at the moment\n",
    "\n",
    "# Primer order check for coverage\n",
    "primer_order_check = True\n",
    "primer_proximity_check = True\n",
    "proximity_max_footprint = 500 ## 500 for LAMP to give buffer\n",
    "\n",
    "# Primer Tm Calculation (Na/Mg = mM) (DNA = nM)\n",
    "na_concentration = 60  # 60 mM\n",
    "primer_concentration = 1000  # 1 µM = 1000 nM\n",
    "mg_concentration = 8  # 8 mM\n",
    "dntp_concentration = 1.4 # 1.4 mM\n",
    "corrected_mg_concentration = max(0, mg_concentration - dntp_concentration)\n",
    "\n",
    "## Expected primer orders (allows both backwards and forwards). This is the first word of the primer name (ex. F1 T7 - \"F1\" is primer name)\n",
    "expected_orders = [\n",
    "    ['F2', 'FL', 'F1', 'B1', 'BL', 'B2'],\n",
    "    ['F3', 'F2', 'FL', 'F1', 'B1', 'BL', 'B2', 'B3'],\n",
    "    ['F3', 'F2', 'FL', 'F1', 'B1', 'B2', 'B3'],\n",
    "    ['FP', 'Probe', 'RP'],\n",
    "    ['FP', 'RP']\n",
    "]\n",
    "\n",
    "# Below are the three test cases included in this tutorial\n",
    "# 1. Chain LAMP with a small set of sequences\n",
    "test_cases = [\n",
    "    {\n",
    "        \"output_name_base\": \"240716_Chain_TEST\", # Output name\n",
    "        \"assays\": \"Targets/targets_Chain_LAMP.csv\", # list of assays. see sample file.  This points towards .fasta files organized by assay with primers\n",
    "        \"alignments\": \"Alignments/Subtypes_Pol_TEST.csv\", # List of subtypes\n",
    "        \"rosalind_threshold\": 3,\n",
    "        \"testing\": True # True disables global coverage weighting\n",
    "    }]\n",
    "# # 2. Chain LAMP with a all pol sequences (takes ~ 30 minutes to run)\n",
    "# test_cases = [\n",
    "#     {\n",
    "#         \"output_name_base\": \"240716_Chain\", # Output name\n",
    "#         \"assays\": \"Targets/targets_Chain_LAMP.csv\", # list of assays. see sample file.  This points towards .fasta files organized by assay with primers\n",
    "#         \"alignments\": \"Alignments/Subtypes_Pol.csv\", # List of subtypes\n",
    "#         \"rosalind_threshold\": 3,\n",
    "#         \"testing\": False\n",
    "#     }]\n",
    "# # 3. Mock validation set with known outputs\n",
    "# test_cases = [\n",
    "#     {\n",
    "#         \"output_name_base\": \"Validation\", # Output name\n",
    "#         \"assays\": \"Targets/240710_Test.csv\", # list of assays. see sample file.  This points towards .fasta files organized by assay with primers\n",
    "#         \"alignments\": \"Alignments/Subtypes_240710_Test.csv\", # List of subtypes\n",
    "#         \"rosalind_threshold\": 3,\n",
    "#         \"testing\": True\n",
    "#     }]\n",
    "# ### Validation - expected outcome should align with Validation_ExpectedOutput.csv\n",
    "# Please set the following to false for the validation\n",
    "# ### primer_order_check = False\n",
    "# ### primer_proximity_check = False\n",
    "\n",
    "print(test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c730215-5877-46df-b5d7-5c3f64a5e013",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Name Base: Validation\n",
      "Assays: Targets/240710_Test.csv\n",
      "Alignments: Alignments/Subtypes_240710_Test.csv\n",
      "Rosalind Threshold: 3\n",
      "---\n",
      "Analyzing assay: Targets/240710_Test.csv\n",
      "Analyzing subtypes: Alignments/Subtypes_240710_Test.csv\n",
      "Processing subtype 240710_TestA.fasta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|██████████| 5/5 [00:00<00:00, 861.15iter/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subtype 240710_TestB.fasta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|██████████| 7/7 [00:00<00:00, 2401.45iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs/Validation_aggregate.json\n",
      "{\n",
      "    \"Validation\": {\n",
      "        \"240710_TestA.fasta\": {\n",
      "            \"ROSALIND_Score_count\": 5,\n",
      "            \"Perfect_coverage_count\": 1,\n",
      "            \"Coverage_with_one_mismatch_count\": 4,\n",
      "            \"Coverage_3_count\": 3,\n",
      "            \"Coverage_5_count\": 2,\n",
      "            \"Coverage_with_one_mismatch_per_assay_count\": 5,\n",
      "            \"Total_sequences\": 5\n",
      "        },\n",
      "        \"240710_TestB.fasta\": {\n",
      "            \"ROSALIND_Score_count\": 3,\n",
      "            \"Perfect_coverage_count\": 2,\n",
      "            \"Coverage_with_one_mismatch_count\": 3,\n",
      "            \"Coverage_3_count\": 3,\n",
      "            \"Coverage_5_count\": 3,\n",
      "            \"Coverage_with_one_mismatch_per_assay_count\": 7,\n",
      "            \"Total_sequences\": 7\n",
      "        }\n",
      "    }\n",
      "}\n",
      "CSV file Outputs/Validation_coverage.csv created successfully.\n",
      "DONE\n",
      "Total time: 0.013569116592407227\n",
      "Coverage by subtype with ROSALIND scoring has been calculated and saved to Outputs/Validation_organized_data_primer.json\n",
      "defaultdict(<function sort_mismatch_frequencies_by_position.<locals>.<lambda> at 0x1735e3f60>, {'240710_TestA.fasta': defaultdict(<function sort_mismatch_frequencies_by_position.<locals>.<lambda>.<locals>.<lambda> at 0x1735e2d40>, {'230710_TestCase_1': defaultdict(<function sort_mismatch_frequencies_by_position.<locals>.<lambda>.<locals>.<lambda>.<locals>.<lambda> at 0x1735e2e80>, {'Test_1': OrderedDict([((0, 'A*'), 5), ((1, 'A'), 2), ((1, 'T*'), 3), ((2, 'G*'), 5), ((3, 'C*'), 5), ((4, 'G*'), 5), ((5, 'T*'), 5), ((6, 'A*'), 5), ((7, 'C*'), 5), ((8, 'C*'), 5), ((9, 'A'), 1), ((9, 'G*'), 4), ((10, 'T*'), 5), ((11, 'A*'), 4), ((11, 'T'), 1), ((12, 'C*'), 5), ((13, 'C'), 1), ((13, 'G*'), 4), ((14, 'T*'), 5), ((15, 'A*'), 5)])}), '230710_TestCase_2': defaultdict(<function sort_mismatch_frequencies_by_position.<locals>.<lambda>.<locals>.<lambda>.<locals>.<lambda> at 0x1735e3c40>, {'Test_2': OrderedDict([((0, 'A'), 1), ((0, 'T'), 4), ((1, 'A'), 2), ((1, 'A*'), 2), ((1, 'T'), 1), ((2, 'C*'), 4), ((2, 'G'), 1), ((3, 'C'), 1), ((3, 'G*'), 4), ((4, 'C'), 4), ((4, 'G'), 1), ((5, 'A'), 4), ((5, 'T*'), 1), ((6, 'A'), 1), ((6, 'T'), 4), ((7, 'C*'), 5), ((8, 'C'), 1), ((8, 'G'), 4), ((9, 'C'), 3), ((9, 'G'), 1), ((9, 'T'), 1), ((10, 'A'), 4), ((10, 'T*'), 1), ((11, 'A*'), 2), ((11, 'T'), 3), ((12, 'C'), 1), ((12, 'G*'), 4), ((13, 'C*'), 5), ((14, 'T*'), 5), ((15, 'A*'), 5)])})}), '240710_TestB.fasta': defaultdict(<function sort_mismatch_frequencies_by_position.<locals>.<lambda>.<locals>.<lambda> at 0x1735e3ba0>, {'230710_TestCase_1': defaultdict(<function sort_mismatch_frequencies_by_position.<locals>.<lambda>.<locals>.<lambda>.<locals>.<lambda> at 0x1735e3b00>, {'Test_1': OrderedDict([((0, 'A*'), 3), ((0, 'C'), 1), ((0, 'T'), 3), ((1, 'A'), 4), ((1, 'G'), 1), ((1, 'T*'), 2), ((2, 'A'), 1), ((2, 'C'), 3), ((2, 'G*'), 2), ((2, 'T'), 1), ((3, 'A'), 2), ((3, 'C*'), 2), ((3, 'G'), 3), ((4, 'A'), 2), ((4, 'C'), 1), ((4, 'G'), 1), ((4, 'G*'), 1), ((4, 'T'), 2), ((5, 'A'), 3), ((5, 'C'), 1), ((5, 'T*'), 3), ((6, 'A'), 1), ((6, 'A*'), 2), ((6, 'C'), 2), ((6, 'G'), 2), ((7, 'A'), 1), ((7, 'C*'), 3), ((7, 'G'), 2), ((7, 'T'), 1), ((8, 'A'), 3), ((8, 'C*'), 1), ((8, 'G'), 3), ((9, 'A'), 1), ((9, 'C'), 3), ((9, 'G'), 1), ((9, 'T'), 2), ((10, 'A'), 3), ((10, 'C'), 1), ((10, 'G'), 1), ((10, 'T*'), 2), ((11, 'A*'), 4), ((11, 'C'), 1), ((11, 'T'), 2), ((12, 'A'), 2), ((12, 'C*'), 3), ((12, 'G'), 1), ((12, 'T'), 1), ((13, 'A'), 2), ((13, 'C'), 2), ((13, 'G*'), 2), ((13, 'T'), 1), ((14, 'A'), 2), ((14, 'C'), 2), ((14, 'T*'), 3), ((15, 'A*'), 4), ((15, 'G'), 2), ((15, 'T'), 1)])}), '230710_TestCase_2': defaultdict(<function sort_mismatch_frequencies_by_position.<locals>.<lambda>.<locals>.<lambda>.<locals>.<lambda> at 0x1735e3a60>, {'Test_2': OrderedDict([((0, 'A'), 1), ((0, 'G'), 1), ((0, 'T*'), 5), ((1, 'A*'), 6), ((1, 'C'), 1), ((2, 'A'), 2), ((2, 'C*'), 5), ((3, 'A'), 1), ((3, 'G*'), 5), ((3, 'T'), 1), ((4, 'A*'), 5), ((4, 'C'), 2), ((5, 'A'), 1), ((5, 'G'), 1), ((5, 'T*'), 5), ((6, 'A'), 2), ((6, 'G*'), 4), ((6, 'T'), 1), ((7, 'A'), 2), ((7, 'C*'), 5), ((8, 'A'), 2), ((8, 'G*'), 4), ((8, 'T'), 1), ((9, 'A'), 2), ((9, 'C*'), 4), ((9, 'T'), 1), ((10, 'A'), 1), ((10, 'G'), 2), ((10, 'T*'), 4), ((11, 'A*'), 5), ((11, 'C'), 2), ((12, 'A'), 1), ((12, 'C'), 1), ((12, 'G*'), 3), ((12, 'T'), 2), ((13, 'A'), 3), ((13, 'C*'), 4), ((14, 'A'), 1), ((14, 'C'), 1), ((14, 'G'), 1), ((14, 'T*'), 4), ((15, 'A*'), 5), ((15, 'C'), 1), ((15, 'G'), 1)])})})})\n",
      "Subtype: 240710_TestA.fasta\n",
      "  Assay: 230710_TestCase_1\n",
      "    Primer ID: Test_1\n",
      "      Position 0:\n",
      "        Base A*: 100.00%\n",
      "      Position 1:\n",
      "        Base A: 40.00%\n",
      "        Base T*: 60.00%\n",
      "      Position 2:\n",
      "        Base G*: 100.00%\n",
      "      Position 3:\n",
      "        Base C*: 100.00%\n",
      "      Position 4:\n",
      "        Base G*: 100.00%\n",
      "      Position 5:\n",
      "        Base T*: 100.00%\n",
      "      Position 6:\n",
      "        Base A*: 100.00%\n",
      "      Position 7:\n",
      "        Base C*: 100.00%\n",
      "      Position 8:\n",
      "        Base C*: 100.00%\n",
      "      Position 9:\n",
      "        Base A: 20.00%\n",
      "        Base G*: 80.00%\n",
      "      Position 10:\n",
      "        Base T*: 100.00%\n",
      "      Position 11:\n",
      "        Base A*: 80.00%\n",
      "        Base T: 20.00%\n",
      "      Position 12:\n",
      "        Base C*: 100.00%\n",
      "      Position 13:\n",
      "        Base C: 20.00%\n",
      "        Base G*: 80.00%\n",
      "      Position 14:\n",
      "        Base T*: 100.00%\n",
      "      Position 15:\n",
      "        Base A*: 100.00%\n",
      "  Assay: 230710_TestCase_2\n",
      "    Primer ID: Test_2\n",
      "      Position 0:\n",
      "        Base A: 20.00%\n",
      "        Base T: 80.00%\n",
      "      Position 1:\n",
      "        Base A: 40.00%\n",
      "        Base A*: 40.00%\n",
      "        Base T: 20.00%\n",
      "      Position 2:\n",
      "        Base C*: 80.00%\n",
      "        Base G: 20.00%\n",
      "      Position 3:\n",
      "        Base C: 20.00%\n",
      "        Base G*: 80.00%\n",
      "      Position 4:\n",
      "        Base C: 80.00%\n",
      "        Base G: 20.00%\n",
      "      Position 5:\n",
      "        Base A: 80.00%\n",
      "        Base T*: 20.00%\n",
      "      Position 6:\n",
      "        Base A: 20.00%\n",
      "        Base T: 80.00%\n",
      "      Position 7:\n",
      "        Base C*: 100.00%\n",
      "      Position 8:\n",
      "        Base C: 20.00%\n",
      "        Base G: 80.00%\n",
      "      Position 9:\n",
      "        Base C: 60.00%\n",
      "        Base G: 20.00%\n",
      "        Base T: 20.00%\n",
      "      Position 10:\n",
      "        Base A: 80.00%\n",
      "        Base T*: 20.00%\n",
      "      Position 11:\n",
      "        Base A*: 40.00%\n",
      "        Base T: 60.00%\n",
      "      Position 12:\n",
      "        Base C: 20.00%\n",
      "        Base G*: 80.00%\n",
      "      Position 13:\n",
      "        Base C*: 100.00%\n",
      "      Position 14:\n",
      "        Base T*: 100.00%\n",
      "      Position 15:\n",
      "        Base A*: 100.00%\n",
      "Subtype: 240710_TestB.fasta\n",
      "  Assay: 230710_TestCase_1\n",
      "    Primer ID: Test_1\n",
      "      Position 0:\n",
      "        Base A*: 42.86%\n",
      "        Base C: 14.29%\n",
      "        Base T: 42.86%\n",
      "      Position 1:\n",
      "        Base A: 57.14%\n",
      "        Base G: 14.29%\n",
      "        Base T*: 28.57%\n",
      "      Position 2:\n",
      "        Base A: 14.29%\n",
      "        Base C: 42.86%\n",
      "        Base G*: 28.57%\n",
      "        Base T: 14.29%\n",
      "      Position 3:\n",
      "        Base A: 28.57%\n",
      "        Base C*: 28.57%\n",
      "        Base G: 42.86%\n",
      "      Position 4:\n",
      "        Base A: 28.57%\n",
      "        Base C: 14.29%\n",
      "        Base G: 14.29%\n",
      "        Base G*: 14.29%\n",
      "        Base T: 28.57%\n",
      "      Position 5:\n",
      "        Base A: 42.86%\n",
      "        Base C: 14.29%\n",
      "        Base T*: 42.86%\n",
      "      Position 6:\n",
      "        Base A: 14.29%\n",
      "        Base A*: 28.57%\n",
      "        Base C: 28.57%\n",
      "        Base G: 28.57%\n",
      "      Position 7:\n",
      "        Base A: 14.29%\n",
      "        Base C*: 42.86%\n",
      "        Base G: 28.57%\n",
      "        Base T: 14.29%\n",
      "      Position 8:\n",
      "        Base A: 42.86%\n",
      "        Base C*: 14.29%\n",
      "        Base G: 42.86%\n",
      "      Position 9:\n",
      "        Base A: 14.29%\n",
      "        Base C: 42.86%\n",
      "        Base G: 14.29%\n",
      "        Base T: 28.57%\n",
      "      Position 10:\n",
      "        Base A: 42.86%\n",
      "        Base C: 14.29%\n",
      "        Base G: 14.29%\n",
      "        Base T*: 28.57%\n",
      "      Position 11:\n",
      "        Base A*: 57.14%\n",
      "        Base C: 14.29%\n",
      "        Base T: 28.57%\n",
      "      Position 12:\n",
      "        Base A: 28.57%\n",
      "        Base C*: 42.86%\n",
      "        Base G: 14.29%\n",
      "        Base T: 14.29%\n",
      "      Position 13:\n",
      "        Base A: 28.57%\n",
      "        Base C: 28.57%\n",
      "        Base G*: 28.57%\n",
      "        Base T: 14.29%\n",
      "      Position 14:\n",
      "        Base A: 28.57%\n",
      "        Base C: 28.57%\n",
      "        Base T*: 42.86%\n",
      "      Position 15:\n",
      "        Base A*: 57.14%\n",
      "        Base G: 28.57%\n",
      "        Base T: 14.29%\n",
      "  Assay: 230710_TestCase_2\n",
      "    Primer ID: Test_2\n",
      "      Position 0:\n",
      "        Base A: 14.29%\n",
      "        Base G: 14.29%\n",
      "        Base T*: 71.43%\n",
      "      Position 1:\n",
      "        Base A*: 85.71%\n",
      "        Base C: 14.29%\n",
      "      Position 2:\n",
      "        Base A: 28.57%\n",
      "        Base C*: 71.43%\n",
      "      Position 3:\n",
      "        Base A: 14.29%\n",
      "        Base G*: 71.43%\n",
      "        Base T: 14.29%\n",
      "      Position 4:\n",
      "        Base A*: 71.43%\n",
      "        Base C: 28.57%\n",
      "      Position 5:\n",
      "        Base A: 14.29%\n",
      "        Base G: 14.29%\n",
      "        Base T*: 71.43%\n",
      "      Position 6:\n",
      "        Base A: 28.57%\n",
      "        Base G*: 57.14%\n",
      "        Base T: 14.29%\n",
      "      Position 7:\n",
      "        Base A: 28.57%\n",
      "        Base C*: 71.43%\n",
      "      Position 8:\n",
      "        Base A: 28.57%\n",
      "        Base G*: 57.14%\n",
      "        Base T: 14.29%\n",
      "      Position 9:\n",
      "        Base A: 28.57%\n",
      "        Base C*: 57.14%\n",
      "        Base T: 14.29%\n",
      "      Position 10:\n",
      "        Base A: 14.29%\n",
      "        Base G: 28.57%\n",
      "        Base T*: 57.14%\n",
      "      Position 11:\n",
      "        Base A*: 71.43%\n",
      "        Base C: 28.57%\n",
      "      Position 12:\n",
      "        Base A: 14.29%\n",
      "        Base C: 14.29%\n",
      "        Base G*: 42.86%\n",
      "        Base T: 28.57%\n",
      "      Position 13:\n",
      "        Base A: 42.86%\n",
      "        Base C*: 57.14%\n",
      "      Position 14:\n",
      "        Base A: 14.29%\n",
      "        Base C: 14.29%\n",
      "        Base G: 14.29%\n",
      "        Base T*: 57.14%\n",
      "      Position 15:\n",
      "        Base A*: 71.43%\n",
      "        Base C: 14.29%\n",
      "        Base G: 14.29%\n",
      "Percentages saved to Outputs/Validation_by_base.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" [1B] Coverage Software - Running the actual code to determine coverage \"\"\"\n",
    "\n",
    "from Bio.SeqUtils import MeltingTemp as mt\n",
    "import json\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "iupac_codes = {\n",
    "    'A': {'A'}, 'C': {'C'}, 'G': {'G'}, 'T': {'T'},\n",
    "    'R': {'A', 'G'}, 'Y': {'C', 'T'}, 'S': {'G', 'C'}, 'W': {'A', 'T'},\n",
    "    'K': {'G', 'T'}, 'M': {'A', 'C'}, 'B': {'C', 'G', 'T'}, 'D': {'A', 'G', 'T'},\n",
    "    'H': {'A', 'C', 'T'}, 'V': {'A', 'C', 'G'}, 'N': {'A', 'C', 'G', 'T'}\n",
    "}\n",
    "\n",
    "def read_fasta(file):\n",
    "    return list(SeqIO.parse(file, \"fasta\"))\n",
    "\n",
    "def reverse_complement(seq):\n",
    "    return str(Seq(seq).reverse_complement())\n",
    "\n",
    "def find_mismatches(seq, primer, start_pos, alignment_type):\n",
    "    \"\"\"\n",
    "    Find mismatches between a sequence segment and a primer.\n",
    "    Args:\n",
    "        seq (str): The target sequence.\n",
    "        primer (str): The primer sequence.\n",
    "        start_pos (int): Starting position of the segment in the target sequence.  \n",
    "    Returns:\n",
    "        List[Tuple[int, int, str, str]]: List of mismatches as (target_location, primer_location, target_base, primer_base).\n",
    "    \"\"\"\n",
    "    segment = seq[start_pos:start_pos + len(primer)]\n",
    "    if alignment_type == \"RC\": ## Adjust the location of mismatch for RC\n",
    "        mismatches = [(start_pos + j, len(primer) - 1 - j, a, b) for j, (a, b) in enumerate(zip(segment, primer))\n",
    "                  if a not in iupac_codes.get(b, {b})]\n",
    "    else:\n",
    "        mismatches = [(start_pos + j, j, a, b) for j, (a, b) in enumerate(zip(segment, primer))\n",
    "                    if a not in iupac_codes.get(b, {b})]\n",
    "    return mismatches # (target_location, primer_location, target_base, primer_base)\n",
    "\n",
    "def find_best_match(seq, primer):\n",
    "    \"\"\"\n",
    "    Find the best match for a primer in a sequence, considering mismatches.\n",
    "    Args:\n",
    "        seq (str): The target sequence.\n",
    "        primer (str): The primer sequence.\n",
    "    Returns:\n",
    "        Tuple[int, List[Tuple[int, int, str, str]], str, int]: Best match position, mismatches, alignment type, and number of mismatches.\n",
    "    \"\"\"\n",
    "    best_mismatches = len(primer) + 1\n",
    "    best_position = -1\n",
    "    best_mismatches_details = []\n",
    "    best_alignment_type = \"primer\"  # \"primer\" or \"RC\"\n",
    "\n",
    "    for alignment_type, current_primer in [(\"primer\", primer), (\"RC\", reverse_complement(primer))]:\n",
    "        for i in range(len(seq) - len(current_primer) + 1):\n",
    "            segment = seq[i:i + len(current_primer)]\n",
    "            mismatch_count = sum(1 for a, b in zip(segment, current_primer) if a not in iupac_codes.get(b, {b}))\n",
    "            if mismatch_count < best_mismatches:\n",
    "                best_mismatches = mismatch_count\n",
    "                best_position = i\n",
    "                best_mismatches_details = find_mismatches(seq, current_primer, i, alignment_type)\n",
    "                best_alignment_type = alignment_type\n",
    "\n",
    "    return best_position, best_mismatches_details, best_alignment_type, best_mismatches\n",
    "\n",
    "def organize_hierarchically(data):\n",
    "    \"\"\"\n",
    "    Organize data hierarchically by file name, subtype, sequence ID, and assay name.\n",
    "    Args:\n",
    "        data (List[Tuple]): List of data tuples to organize.\n",
    "    Returns:\n",
    "        Dict: Hierarchically organized data.\n",
    "    \"\"\"\n",
    "    organized_data = {}\n",
    "\n",
    "    for entry in data:\n",
    "        file_name, subtype, assay_name, sequence_id, primer_id, primer_length, best_position, mismatches, alignment_type, num_mismatches, primer_seq = entry\n",
    "        if file_name not in organized_data:\n",
    "            organized_data[file_name] = {}\n",
    "        if subtype not in organized_data[file_name]:\n",
    "            organized_data[file_name][subtype] = {}\n",
    "        if sequence_id not in organized_data[file_name][subtype]:\n",
    "            organized_data[file_name][subtype][sequence_id] = {}\n",
    "        if assay_name not in organized_data[file_name][subtype][sequence_id]:\n",
    "            organized_data[file_name][subtype][sequence_id][assay_name] = []\n",
    "        organized_data[file_name][subtype][sequence_id][assay_name].append({\n",
    "            \"Primer_ID\": primer_id,\n",
    "            \"Primer_length\": primer_length,\n",
    "            \"Best_Position\": best_position,\n",
    "            \"Mismatches\": mismatches,\n",
    "            \"Alignment_Type\": alignment_type,\n",
    "            \"Num_Mismatches\": num_mismatches,\n",
    "            \"Primer_Sequence\": primer_seq\n",
    "        })\n",
    "\n",
    "    return organized_data\n",
    "\n",
    "def perfect_coverage(organized_data):\n",
    "    \"\"\"\n",
    "    Calculate perfect coverage and coverage with one mismatch for each entry in the data.\n",
    "    Args:\n",
    "        organized_data (Dict): Hierarchically organized data. \n",
    "    Returns:\n",
    "        Dict: Data with added coverage information.\n",
    "    \"\"\"\n",
    "    for file_name, subtypes in organized_data.items():\n",
    "        for subtype, sequences in subtypes.items():\n",
    "            for sequence_id, assays in sequences.items():\n",
    "                for assay_name, entries in assays.items():\n",
    "                    for entry in entries:\n",
    "                        # Add Coverage field based on Num_Mismatches\n",
    "                        entry['Perfect_coverage'] = 1 if entry['Num_Mismatches'] == 0 else 0\n",
    "                        entry['Coverage_with_one_mismatch'] = 1 if entry['Num_Mismatches'] <= 1 else 0\n",
    "    return organized_data\n",
    "\n",
    "def coverage_with_one_mismatch_n_bases_from_end(organized_data, n):\n",
    "    \"\"\"\n",
    "    Calculate coverage with one mismatch within n bases from the end for each entry in the data.\n",
    "    Args:\n",
    "        organized_data (Dict): Hierarchically organized data.\n",
    "        n (int): Number of bases from the end to consider. \n",
    "    Returns:\n",
    "        Dict: Data with added coverage information.\n",
    "    \"\"\"\n",
    "    coverage_field_name = f\"Coverage_{n}\"\n",
    "    for file_name, subtypes in organized_data.items():\n",
    "        for subtype, sequences in subtypes.items():\n",
    "            for sequence_id, assays in sequences.items():\n",
    "                for assay_name, entries in assays.items():\n",
    "                    for entry in entries:\n",
    "                        mismatches = entry['Mismatches']\n",
    "                        primer_length = entry['Primer_length']\n",
    "                        mismatch_within_n_bases = False\n",
    "\n",
    "                        if len(mismatches) == 1:\n",
    "                            mismatch_position = mismatches[0][1]\n",
    "                            if primer_length - mismatch_position <= n:\n",
    "                                mismatch_within_n_bases = True\n",
    "\n",
    "                        if len(mismatches) == 0 or (len(mismatches) == 1 and not mismatch_within_n_bases):\n",
    "                            entry[coverage_field_name] = 1\n",
    "                        else:\n",
    "                            entry[coverage_field_name] = 0\n",
    "    return organized_data\n",
    "\n",
    "def coverage_with_one_mismatch_per_assay(organized_data):\n",
    "    \"\"\"\n",
    "    Calculate coverage with one mismatch per assay for each entry in the data.\n",
    "    Args:\n",
    "        organized_data (Dict): Hierarchically organized data.\n",
    "    Returns:\n",
    "        Dict: Data with added coverage information.\n",
    "    \"\"\"\n",
    "    for file_name, subtypes in organized_data.items():\n",
    "        for subtype, sequences in subtypes.items():\n",
    "            for sequence_id, assays in sequences.items():\n",
    "                for assay_name, entries in assays.items():\n",
    "                    num_entries_with_no_mismatches = sum(1 for entry in entries if entry['Num_Mismatches'] == 0)\n",
    "                    num_entries_with_one_mismatch = sum(1 for entry in entries if entry['Num_Mismatches'] == 1)\n",
    "                    # Up to one entry with a mismatch while all others have none\n",
    "                    if num_entries_with_one_mismatch <= 1 and num_entries_with_no_mismatches >= len(entries) - 1:\n",
    "                        for entry in entries:\n",
    "                            entry['Coverage_with_one_mismatch_per_assay'] = 1\n",
    "                    else:\n",
    "                        for entry in entries:\n",
    "                            entry['Coverage_with_one_mismatch_per_assay'] = 0\n",
    "    return organized_data\n",
    "\n",
    "def calculate_assay_score(organized_data):\n",
    "    \"\"\"\n",
    "    Calculate ROSALIND score for each primer based on mismatch positions and add it to the data.\n",
    "    ROSALIND: https://www.rosalind.bio/en/knowledge/calculating-the-severity-score-for-lamp-assays\n",
    "    Args:\n",
    "        organized_data (Dict): Hierarchically organized data.\n",
    "    Returns:\n",
    "        Dict: Data with added primer scores.\n",
    "    \"\"\"\n",
    "\n",
    "    for file_name, subtypes in organized_data.items():\n",
    "        for subtype, sequences in subtypes.items():\n",
    "            for sequence_id, assays in sequences.items():\n",
    "                for assay_name, entries in assays.items():\n",
    "                    #print(assay_name)\n",
    "                    for entry in entries:\n",
    "                        #print(entry['Primer_ID'])\n",
    "                        score = 0\n",
    "                        mismatches = entry['Mismatches']\n",
    "                        primer_length = entry['Primer_length']\n",
    "                        total_mismatches = entry['Num_Mismatches']\n",
    "                        sequence = entry['Primer_Sequence']\n",
    "                        old_sequence = correct_mismatches(entry['Primer_Sequence'], entry['Mismatches'], entry['Alignment_Type'])\n",
    "\n",
    "                        # Score +2 for mismatches that are next to each other\n",
    "                        for i in range(len(mismatches) - 1):\n",
    "                            if mismatches[i][1] + 1 == mismatches[i + 1][1]:\n",
    "                                score += 2\n",
    "                                #print(\"+2 Next\")\n",
    "\n",
    "                        # Score +3 per mismatch in the last 2 bases\n",
    "                        for mismatch in mismatches:\n",
    "                            mismatch_position = mismatch[1]\n",
    "                            if primer_length - mismatch_position <= 2:\n",
    "                                score += 3\n",
    "                                #print(\"+3 Last2\")\n",
    "\n",
    "                        # Score +2 per mismatch in positions 3-5 from the end\n",
    "                        for mismatch in mismatches:\n",
    "                            mismatch_position = mismatch[1]\n",
    "                            if 3 <= primer_length - mismatch_position <= 5:\n",
    "                                score += 2\n",
    "                                #print(\"+2 Last 5\")\n",
    "                        \n",
    "                        # Calculate Tm difference and adjust score\n",
    "                        tm_diff = calculate_tm_diff(old_sequence, sequence)\n",
    "                        if abs(tm_diff) > 5:\n",
    "                            score += 4\n",
    "                            #print(\"+4 TM\")\n",
    "                        elif abs(tm_diff) > 2.5:\n",
    "                            score += 2\n",
    "                            #print(\"+2 TM\")\n",
    "\n",
    "                        # Calculate score based on total number of mismatches and other criteria\n",
    "                        entry['ROSALIND_Score'] = calculate_mismatch_score(total_mismatches) + score\n",
    "                        #print(\"MS\" + str(total_mismatches))\n",
    "                        #print(calculate_mismatch_score(total_mismatches))\n",
    "\n",
    "    return organized_data\n",
    "\n",
    "def calculate_mismatch_score(mismatches):\n",
    "    \"\"\"\n",
    "    Calculate the mismatch score (within ROSALIND) based on the total number of mismatches.\n",
    "    ROSALIND: https://www.rosalind.bio/en/knowledge/calculating-the-severity-score-for-lamp-assays\n",
    "    Args:\n",
    "        mismatches (int): Total number of mismatches.   \n",
    "    Returns:\n",
    "        int: Mismatch score.\n",
    "    \"\"\"\n",
    "    #print(\"INPUT\" + str(mismatches))\n",
    "    if mismatches == 0 or mismatches == 1:\n",
    "        return 0\n",
    "    elif mismatches == 2:\n",
    "        return 1\n",
    "    elif mismatches == 3:\n",
    "        return 2\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "def aggregate_coverage(organized_data):\n",
    "    \"\"\"\n",
    "    Aggregate coverage information from the organized data.\n",
    "    Args:\n",
    "        organized_data (Dict): Hierarchically organized data.  \n",
    "    Returns:\n",
    "        Dict: Aggregated coverage information.\n",
    "    \"\"\"\n",
    "    aggregated_data = {}\n",
    "\n",
    "    for file_name, subtypes in organized_data.items():\n",
    "        for subtype, sequences in subtypes.items():\n",
    "            for sequence_id, assays in sequences.items():\n",
    "                for assay_name, entries in assays.items():\n",
    "                    max_score = max(entry['ROSALIND_Score'] for entry in entries)\n",
    "                    representative_score = min(max_score, 5)\n",
    "                    ##representative_score = entries[0]['ROSALIND_Score'] if entries else 0 ## Old version with score for whole assay\n",
    "\n",
    "                    perfect_coverage = all(entry['Perfect_coverage'] == 1 for entry in entries)\n",
    "                    coverage_with_one_mismatch = all(entry['Coverage_with_one_mismatch'] == 1 for entry in entries)\n",
    "                    coverage_3 = all(entry['Coverage_3'] == 1 for entry in entries)\n",
    "                    coverage_5 = all(entry['Coverage_5'] == 1 for entry in entries)\n",
    "                    coverage_with_one_mismatch_per_assay = all(entry['Coverage_with_one_mismatch_per_assay'] == 1 for entry in entries)\n",
    "\n",
    "                    primers_close = all(entry['Primers_Close_Enough'] for entry in entries)\n",
    "                    primers_ordered = all(entry['Primers_Ordered_Correctly'] for entry in entries)\n",
    "\n",
    "                    \n",
    "                    if file_name not in aggregated_data:\n",
    "                        aggregated_data[file_name] = {}\n",
    "                    if subtype not in aggregated_data[file_name]:\n",
    "                        aggregated_data[file_name][subtype] = {}\n",
    "                    if sequence_id not in aggregated_data[file_name][subtype]:\n",
    "                        aggregated_data[file_name][subtype][sequence_id] = {}\n",
    "                    \n",
    "                    aggregated_data[file_name][subtype][sequence_id][assay_name] = {\n",
    "                        \"ROSALIND_Score\": representative_score,\n",
    "                        \"Perfect_coverage\": int(perfect_coverage),\n",
    "                        \"Coverage_with_one_mismatch\": int(coverage_with_one_mismatch),\n",
    "                        \"Coverage_3\": int(coverage_3),\n",
    "                        \"Coverage_5\": int(coverage_5),\n",
    "                        \"Coverage_with_one_mismatch_per_assay\": int(coverage_with_one_mismatch_per_assay),\n",
    "                        \"Primers_Close_Enough\": primers_close,\n",
    "                        \"Primers_Ordered_Correctly\": primers_ordered\n",
    "                    }\n",
    "    \n",
    "    return aggregated_data\n",
    "\n",
    "def update_json_file(file_path, new_data):\n",
    "    \"\"\"\n",
    "    Update an existing JSON file with new data.\n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "        new_data (Dict): New data to update in the JSON file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True) # Ensure the directory exists\n",
    "\n",
    "    # Read the existing file if it exists\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            existing_data = json.load(file)\n",
    "    else:\n",
    "        existing_data = {}\n",
    "\n",
    "    # Update the existing data with new aggregated data\n",
    "    for key, value in new_data.items():\n",
    "        if key in existing_data:\n",
    "            existing_data[key].update(value)\n",
    "        else:\n",
    "            existing_data[key] = value\n",
    "\n",
    "    # Write the updated data back to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(existing_data, file, indent=4)\n",
    "\n",
    "def assay_coverage_analysis(file_path, rosalind_threshold):\n",
    "    \"\"\"\n",
    "    Create the final nested dictionary with assay coverage analysis.\n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "        rosalind_threshold (int): Threshold for the ROSALIND score.  \n",
    "    Returns:\n",
    "        Dict: Final nested dictionary with assay coverage analysis.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    final_dict = {}\n",
    "    \n",
    "    for file_name, subtypes in data.items():\n",
    "        if file_name not in final_dict:\n",
    "            final_dict[file_name] = {}\n",
    "        for subtype, sequences in subtypes.items():\n",
    "            if subtype not in final_dict[file_name]:\n",
    "                final_dict[file_name][subtype] = {\n",
    "                    \"ROSALIND_Score_count\": 0,\n",
    "                    \"Perfect_coverage_count\": 0,\n",
    "                    \"Coverage_with_one_mismatch_count\": 0,\n",
    "                    \"Coverage_3_count\": 0,\n",
    "                    \"Coverage_5_count\": 0,\n",
    "                    \"Coverage_with_one_mismatch_per_assay_count\": 0,\n",
    "                    \"Total_sequences\": len(sequences)\n",
    "                }\n",
    "            for sequence_id, assays in sequences.items():\n",
    "                perfect_coverage = False\n",
    "                coverage_with_one_mismatch = False\n",
    "                coverage_3 = False\n",
    "                coverage_5 = False\n",
    "                coverage_ROSALIND = False\n",
    "                coverage_with_one_mismatch_per_assay = False\n",
    "\n",
    "                for entry in assays.values():\n",
    "\n",
    "                    ## Leave as false if primer order check or proximity is turned on and doesn't meet criteria\n",
    "                    if primer_order_check and not entry['Primers_Ordered_Correctly'] == True:\n",
    "                            continue\n",
    "                    if primer_proximity_check and not entry['Primers_Close_Enough'] == True:\n",
    "                            continue\n",
    "                    \n",
    "                    ## Set each of these to true if true for one assay in set\n",
    "                    perfect_coverage = perfect_coverage or entry[\"Perfect_coverage\"] == 1\n",
    "                    coverage_with_one_mismatch = coverage_with_one_mismatch or entry[\"Coverage_with_one_mismatch\"] == 1\n",
    "                    coverage_3 = coverage_3 or entry[\"Coverage_3\"] == 1\n",
    "                    coverage_5 = coverage_5 or entry[\"Coverage_5\"] == 1\n",
    "                    coverage_ROSALIND = coverage_ROSALIND or entry[\"ROSALIND_Score\"] <= rosalind_threshold\n",
    "                    coverage_with_one_mismatch_per_assay = coverage_with_one_mismatch_per_assay or entry[\"Coverage_with_one_mismatch_per_assay\"] == 1\n",
    "\n",
    "                if perfect_coverage:\n",
    "                    final_dict[file_name][subtype][\"Perfect_coverage_count\"] += 1\n",
    "                if coverage_with_one_mismatch:\n",
    "                    final_dict[file_name][subtype][\"Coverage_with_one_mismatch_count\"] += 1\n",
    "                if coverage_3:\n",
    "                    final_dict[file_name][subtype][\"Coverage_3_count\"] += 1\n",
    "                if coverage_5:\n",
    "                    final_dict[file_name][subtype][\"Coverage_5_count\"] += 1\n",
    "                if coverage_with_one_mismatch_per_assay:\n",
    "                    final_dict[file_name][subtype][\"Coverage_with_one_mismatch_per_assay_count\"] += 1\n",
    "                if coverage_ROSALIND:\n",
    "                    final_dict[file_name][subtype][\"ROSALIND_Score_count\"] += 1    \n",
    "    return final_dict\n",
    "\n",
    "def create_csv_from_final_dict(final_dict, output_csv):\n",
    "    categories = [\"ROSALIND_Score_count\", \"Perfect_coverage_count\", \"Coverage_with_one_mismatch_count\", \"Coverage_3_count\", \"Coverage_5_count\", \"Coverage_with_one_mismatch_per_assay_count\"]\n",
    "\n",
    "    # Prepare the data for CSV\n",
    "    rows = []\n",
    "    aggregate_totals = {category: 0 for category in categories}\n",
    "    aggregate_total_sequences = 0\n",
    "\n",
    "    for file_name, subtypes in final_dict.items():\n",
    "        for subtype, metrics in subtypes.items():\n",
    "            row = [subtype]\n",
    "            total_sequences = metrics[\"Total_sequences\"]\n",
    "            aggregate_total_sequences += total_sequences\n",
    "\n",
    "            for category in categories:\n",
    "                count = metrics[category]\n",
    "                aggregate_totals[category] += count\n",
    "                percent = (count / total_sequences) * 100 if total_sequences > 0 else 0\n",
    "                row.append(count)\n",
    "                row.append(f\"{percent:.1f}%\")\n",
    "            row.append(total_sequences)  # Add the total sequences to the row\n",
    "            rows.append(row)\n",
    "    \n",
    "    # Calculate aggregated row\n",
    "    aggregate_row = [\"Aggregated\"]\n",
    "    for category in categories:\n",
    "        count = aggregate_totals[category]\n",
    "        percent = (count / aggregate_total_sequences) * 100 if aggregate_total_sequences > 0 else 0\n",
    "        aggregate_row.append(count)\n",
    "        aggregate_row.append(f\"{percent:.1f}%\")\n",
    "    aggregate_row.append(aggregate_total_sequences)  # Add the total sequences to the aggregated row\n",
    "\n",
    "    # Write the data to CSV\n",
    "    with open(output_csv, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        header = [\"Subtype\"]\n",
    "        for category in categories:\n",
    "            header.append(f\"{category.replace('_count', '')} (Count)\")\n",
    "            header.append(f\"{category.replace('_count', '')} (%)\")\n",
    "        header.append(\"Total_sequences\")  # Add the header for total sequences\n",
    "        csvwriter.writerow(header)\n",
    "        csvwriter.writerows(rows)\n",
    "        csvwriter.writerow(aggregate_row)  # Write the aggregated row\n",
    "\n",
    "    print(f\"CSV file {output_csv} created successfully.\")\n",
    "\n",
    "def calculate_weighted_aggregate(global_diversity_csv, output_csv, output_aggregate_csv):\n",
    "    # Load the CSV files\n",
    "    global_diversity_df = pd.read_csv(global_diversity_csv)\n",
    "    output_df = pd.read_csv(output_csv)\n",
    "    input_df = output_df\n",
    "    # Remove the \"Aggregated\" row from the output dataframe\n",
    "    output_df = output_df[output_df['Subtype'] != 'Aggregated']\n",
    "\n",
    "    # Ensure both dataframes have the same number of rows\n",
    "    assert len(global_diversity_df) == len(output_df), \"The number of rows in the two files must match.\"\n",
    "\n",
    "    # Define the categories to be aggregated\n",
    "    categories = [\n",
    "        \"ROSALIND_Score (%)\", \"Perfect_coverage (%)\",\n",
    "        \"Coverage_with_one_mismatch (%)\", \"Coverage_3 (%)\",\n",
    "        \"Coverage_5 (%)\", \"Coverage_with_one_mismatch_per_assay (%)\"\n",
    "    ]\n",
    "    for category in categories:\n",
    "        #output_df[category] = output_df[category].str.rstrip('%').astype('float') / 100.0\n",
    "        output_df.loc[:, category] = output_df[category].str.rstrip('%').astype('float') / 100.0\n",
    "        \n",
    "    # Initialize dictionary to store weighted sums and total prevalence\n",
    "    weighted_sums = {category: 0 for category in categories}\n",
    "    total_prevalence = global_diversity_df['Prevalence'].sum()\n",
    "\n",
    "    # Calculate weighted sums\n",
    "    for index in range(len(global_diversity_df)):\n",
    "        prevalence = global_diversity_df.loc[index, 'Prevalence']\n",
    "        for category in categories:\n",
    "            weighted_sums[category] += output_df.loc[index, category] * prevalence\n",
    "\n",
    "    # Calculate weighted percentages\n",
    "    weighted_aggregate_row = [\"Weighted Aggregated\"]\n",
    "    for category in categories:  # Skip Total_sequences for percentage calculation\n",
    "        weighted_percent = weighted_sums[category] \n",
    "        weighted_aggregate_row.append(f\"{weighted_percent:.1f}%\")\n",
    "    \n",
    "    # Prepare the data for writing to CSV\n",
    "    rows = [row for row in input_df.values]  # Existing rows\n",
    "    print(output_df)\n",
    "    weighted_aggregate_row_format = []\n",
    "    for cell in weighted_aggregate_row:\n",
    "        weighted_aggregate_row_format.append(cell)\n",
    "        weighted_aggregate_row_format.append(\"\")\n",
    "    rows.append(weighted_aggregate_row_format)  # Append the weighted aggregate row\n",
    "\n",
    "    # Write the data to the new CSV\n",
    "    with open(output_aggregate_csv, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        header = output_df.columns.tolist()\n",
    "        csvwriter.writerow(header)\n",
    "        csvwriter.writerows(rows)\n",
    "    \n",
    "    print(f\"CSV file {output_aggregate_csv} created successfully with weighted aggregates.\")\n",
    "\n",
    "def calculate_coverage_by_subtype_primer(organized_data_file, output_file, rosalind_threshold):\n",
    "    \"\"\"\n",
    "    Calculate coverage by subtype for each primer region, including ROSALIND scoring.\n",
    "    \n",
    "    Args:\n",
    "        organized_data_file (str): Path to the JSON file containing organized data.\n",
    "        output_file (str): Path to the output JSON file for saving the coverage analysis results.\n",
    "        rosalind_threshold (int): Threshold for the ROSALIND score. Default is 5.\n",
    "    \n",
    "    Returns: Create organized_data_primer.json file\n",
    "    \"\"\"\n",
    "\n",
    "    with open(organized_data_file, 'r') as file:\n",
    "        organized_data = json.load(file)\n",
    "\n",
    "    coverage_by_subtype = {}\n",
    "\n",
    "    for file_name, subtypes in organized_data.items():\n",
    "        for subtype, sequences in subtypes.items():\n",
    "            if subtype not in coverage_by_subtype:\n",
    "                coverage_by_subtype[subtype] = {}\n",
    "            for sequence_id, assays in sequences.items():\n",
    "                for assay_name, entries in assays.items():\n",
    "                    if assay_name not in coverage_by_subtype[subtype]:\n",
    "                        coverage_by_subtype[subtype][assay_name] = {}\n",
    "                    for entry in entries:\n",
    "                        primer_id = entry['Primer_ID']\n",
    "                        if primer_id not in coverage_by_subtype[subtype][assay_name]:\n",
    "                            coverage_by_subtype[subtype][assay_name][primer_id] = {\n",
    "                                'Perfect_coverage': 0,\n",
    "                                'Coverage_with_one_mismatch': 0,\n",
    "                                'Coverage_3': 0,\n",
    "                                'Coverage_5': 0,\n",
    "                                'Coverage_with_one_mismatch_per_assay': 0,\n",
    "                                'ROSALIND_Score_within_threshold': 0,\n",
    "                                'Total': 0\n",
    "                            }\n",
    "\n",
    "                        coverage_by_subtype[subtype][assay_name][primer_id]['Perfect_coverage'] += entry['Perfect_coverage']\n",
    "                        coverage_by_subtype[subtype][assay_name][primer_id]['Coverage_with_one_mismatch'] += entry['Coverage_with_one_mismatch']\n",
    "                        coverage_by_subtype[subtype][assay_name][primer_id]['Coverage_3'] += entry['Coverage_3']\n",
    "                        coverage_by_subtype[subtype][assay_name][primer_id]['Coverage_5'] += entry['Coverage_5']\n",
    "                        coverage_by_subtype[subtype][assay_name][primer_id]['Coverage_with_one_mismatch_per_assay'] += entry['Coverage_with_one_mismatch_per_assay']\n",
    "                        if entry['ROSALIND_Score'] <= rosalind_threshold:\n",
    "                            coverage_by_subtype[subtype][assay_name][primer_id]['ROSALIND_Score_within_threshold'] += 1\n",
    "                        coverage_by_subtype[subtype][assay_name][primer_id]['Total'] += 1\n",
    "\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(coverage_by_subtype, outfile, indent=4)\n",
    "\n",
    "    print(f\"Coverage by subtype with ROSALIND scoring has been calculated and saved to {output_file}\")\n",
    "\n",
    "def check_primer_proximity_and_order(organized_data):\n",
    "    \"\"\"\n",
    "    Check if all primers in each assay are close enough to each other and in correct order based on 'Best_Position'.\n",
    "    Args:\n",
    "        organized_data (Dict): Hierarchically organized data.\n",
    "        max_distance (int): Maximum allowed distance between primers.\n",
    "    Returns:\n",
    "        Dict: Data with flags indicating if primers are within the maximum distance and in correct order.\n",
    "    \"\"\"\n",
    "    def are_primers_close_enough(primer_positions, max_distance):\n",
    "        for i in range(len(primer_positions)):\n",
    "            for j in range(i + 1, len(primer_positions)):\n",
    "                if abs(primer_positions[i] - primer_positions[j]) > max_distance:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def are_primers_ordered(sorted_primers):\n",
    "        sorted_ids = [primer['Primer_ID'] for primer in sorted_primers]\n",
    "\n",
    "        for expected_order in expected_orders:\n",
    "            if sorted_ids == expected_order or sorted_ids == expected_order[::-1]:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    for file_name, subtypes in organized_data.items():\n",
    "        for subtype, sequences in subtypes.items():\n",
    "            for sequence_id, assays in sequences.items():\n",
    "                for assay_name, entries in assays.items():\n",
    "                    # Sort primers based on Best_Position\n",
    "                    sorted_primers = sorted(entries, key=lambda x: x['Best_Position'])\n",
    "                    \n",
    "                    # Extract the Best_Position values\n",
    "                    primer_positions = [entry['Best_Position'] for entry in sorted_primers]\n",
    "\n",
    "                    # Check if all primers are within the max_distance\n",
    "                    primers_close = are_primers_close_enough(primer_positions, proximity_max_footprint)\n",
    "                    \n",
    "                    # Check if primers are ordered correctly based on 'Primer_ID' and 'Best_Position'\n",
    "                    primers_ordered = are_primers_ordered(sorted_primers)\n",
    "                    \n",
    "                    # Add the flags to indicate primer proximity and order\n",
    "                    for entry in entries:\n",
    "                        entry['Primers_Close_Enough'] = primers_close\n",
    "                        entry['Primers_Ordered_Correctly'] = primers_ordered\n",
    "\n",
    "    return organized_data\n",
    "\n",
    "def correct_mismatches(primer_sequence, mismatches, alignment_type):\n",
    "    \"\"\"\n",
    "    Correct mismatches in the primer sequence to generate the original sequence.\n",
    "    Args:\n",
    "        primer_sequence (str): The primer sequence with mismatches.\n",
    "        mismatches (list): List of mismatches, each defined as [position, index, original_base, mismatched_base].\n",
    "        alignment_type (str): Alignment type, \"RC\" for reverse complement.\n",
    "    Returns:\n",
    "        str: The corrected (original) sequence.\n",
    "    \"\"\"\n",
    "    # If the alignment type is RC, reverse complement the sequence\n",
    "    if alignment_type == \"RC\":\n",
    "        primer_sequence = str(Seq(primer_sequence).reverse_complement())\n",
    "\n",
    "    # Convert to list for mutable string\n",
    "    original_sequence = list(primer_sequence)\n",
    "\n",
    "    # Correct each mismatch\n",
    "    for mismatch in mismatches:\n",
    "        position, index, original_base, mismatched_base = mismatch\n",
    "        \n",
    "        # Adjust index if alignment is RC since we flipped the sequence\n",
    "        if alignment_type == \"RC\":\n",
    "            index = len(primer_sequence) - 1 - index\n",
    "\n",
    "        # Replace the mismatched base with the original base\n",
    "        original_sequence[index] = original_base\n",
    "\n",
    "    # Join the list back into a string\n",
    "    return ''.join(original_sequence)\n",
    "\n",
    "def calculate_tm_diff(primer_sequence, old_primer):\n",
    "    \"\"\"\n",
    "    Calculate the melting temperature (Tm) of a DNA primer using the Biopython library.\n",
    "\n",
    "    Parameters:\n",
    "    primer_sequence (str): The DNA sequence of the primer.\n",
    "    na_concentration (float): The concentration of monovalent cations (e.g., Na+) in M. Default is 50 mM.\n",
    "    primer_concentration (float): The concentration of primers in M. Default is 250 nM.\n",
    "\n",
    "    Returns:\n",
    "    float: The melting temperature (Tm) in degrees Celsius.\n",
    "    \"\"\"\n",
    "    # Calculate the Tm using the nearest-neighbor method provided by Biopython\n",
    "    tm = mt.Tm_NN(primer_sequence, Na=na_concentration, Mg=corrected_mg_concentration, dnac1=primer_concentration)\n",
    "    tm_old = mt.Tm_NN(old_primer, Na=na_concentration, Mg=corrected_mg_concentration, dnac1=primer_concentration)\n",
    "\n",
    "    return tm-tm_old\n",
    "\n",
    "def write_run_info(output_file, output_name_base, assays, alignments, rosalind_threshold,\n",
    "                   testing, globalAnalysis, primer_order_check, primer_proximity_check,\n",
    "                   proximity_max_footprint, na_concentration, primer_concentration,\n",
    "                   mg_concentration, dntp_concentration, corrected_mg_concentration,\n",
    "                   expected_orders):\n",
    "    \"\"\"\n",
    "    Write the run information for the test case to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        output_file (str): The path to the output JSON file.\n",
    "        output_name_base (str): The base name for the output.\n",
    "        assays (str): Path to the assays file.\n",
    "        alignments (str): Path to the alignments file.\n",
    "        rosalind_threshold (int): Threshold for the ROSALIND score.\n",
    "        testing (bool): Indicates if the run is for testing purposes.\n",
    "        globalAnalysis (bool): Indicates if global analysis is enabled.\n",
    "        primer_order_check (bool): Flag to check primer order for coverage.\n",
    "        primer_proximity_check (bool): Flag to check primer proximity for coverage.\n",
    "        proximity_max_footprint (int): Maximum allowed distance for primer proximity.\n",
    "        na_concentration (float): Sodium ion concentration in mM.\n",
    "        primer_concentration (float): Primer concentration in nM.\n",
    "        mg_concentration (float): Magnesium ion concentration in mM.\n",
    "        dntp_concentration (float): dNTP concentration in mM.\n",
    "        corrected_mg_concentration (float): Corrected magnesium ion concentration after accounting for dNTP binding.\n",
    "        expected_orders (list): List of expected primer orders.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    run_info = {\n",
    "        \"output_name_base\": output_name_base,\n",
    "        \"assays\": assays,\n",
    "        \"alignments\": alignments,\n",
    "        \"rosalind_threshold\": rosalind_threshold,\n",
    "        \"testing\": testing,\n",
    "        \"globalAnalysis\": globalAnalysis,\n",
    "        \"primer_order_check\": primer_order_check,\n",
    "        \"primer_proximity_check\": primer_proximity_check,\n",
    "        \"proximity_max_footprint\": proximity_max_footprint,\n",
    "        \"na_concentration\": na_concentration,\n",
    "        \"primer_concentration\": primer_concentration,\n",
    "        \"mg_concentration\": mg_concentration,\n",
    "        \"dntp_concentration\": dntp_concentration,\n",
    "        \"corrected_mg_concentration\": corrected_mg_concentration,\n",
    "        \"expected_orders\": expected_orders\n",
    "    }\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(run_info, f, indent=4)\n",
    "\n",
    "# Dictionary to handle IUPAC nucleotide complements\n",
    "IUPAC_COMPLEMENT = {\n",
    "    'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G',\n",
    "    'R': 'Y', 'Y': 'R', 'S': 'S', 'W': 'W',\n",
    "    'K': 'M', 'M': 'K', 'B': 'V', 'V': 'B',\n",
    "    'D': 'H', 'H': 'D', 'N': 'N'\n",
    "}\n",
    "\n",
    "def process_mismatches(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    mismatch_frequencies = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(int))))\n",
    "\n",
    "    for experiment, subtypes in data.items():\n",
    "        for subtype, sequences in subtypes.items():\n",
    "            for sequence, assays in sequences.items():\n",
    "                for assay_name, primers in assays.items():\n",
    "                    for primer in primers:\n",
    "                        primer_id = primer.get(\"Primer_ID\")\n",
    "                        primer_sequence = primer.get(\"Primer_Sequence\")\n",
    "                        mismatches = primer.get(\"Mismatches\", [])\n",
    "\n",
    "                        # Track mismatched positions\n",
    "                        mismatched_positions = set()\n",
    "\n",
    "                        # Process mismatches\n",
    "                        for mismatch in mismatches:\n",
    "                            position = mismatch[1]\n",
    "                            reference_base = mismatch[3]  # Switch to observed base\n",
    "                            observed_base = mismatch[2]   # Switch to reference base\n",
    "\n",
    "                            if primer[\"Alignment_Type\"] == \"RC\":\n",
    "                                # Apply reverse complement if alignment type is RC\n",
    "                                position = len(primer[\"Primer_Sequence\"]) - 1 - position\n",
    "                                reference_base = reverse_complement(mismatch[3])\n",
    "                                observed_base = reverse_complement(mismatch[2])\n",
    "\n",
    "                            # Mark position as mismatched\n",
    "                            mismatched_positions.add(position)\n",
    "\n",
    "                            # Update the frequency count\n",
    "                            mismatch_frequencies[subtype][assay_name][primer_id][(position, observed_base)] += 1\n",
    "\n",
    "                        # Initialize positions that have no mismatches with the original base\n",
    "                        for i, base in enumerate(primer_sequence):\n",
    "                            if i not in mismatched_positions:\n",
    "                                mismatch_frequencies[subtype][assay_name][primer_id][(i, base+\"*\")] += 1\n",
    "\n",
    "    return mismatch_frequencies\n",
    "\n",
    "def calculate_percentages(mismatch_frequencies):\n",
    "    percentages = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(dict))))\n",
    "\n",
    "    for subtype, assays in mismatch_frequencies.items():\n",
    "        for assay, primers in assays.items():\n",
    "            for primer_id, mismatches in primers.items():\n",
    "                position_totals = defaultdict(int)\n",
    "\n",
    "                # Calculate total occurrences per position\n",
    "                for (position, base), frequency in mismatches.items():\n",
    "                    position_totals[position] += frequency\n",
    "\n",
    "                # Calculate percentages\n",
    "                for (position, base), frequency in mismatches.items():\n",
    "                    total = position_totals[position]\n",
    "                    if total > 0:\n",
    "                        percentage = (frequency / total) * 100\n",
    "                        percentages[subtype][assay][primer_id][position][base] = round(percentage, 2)\n",
    "\n",
    "    return percentages\n",
    "\n",
    "def save_percentages_to_json(percentages, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        json.dump(percentages, file, indent=4)\n",
    "\n",
    "def print_percentages(percentages):\n",
    "    for subtype, assays in percentages.items():\n",
    "        print(f\"Subtype: {subtype}\")\n",
    "        for assay, primers in assays.items():\n",
    "            print(f\"  Assay: {assay}\")\n",
    "            for primer_id, positions in primers.items():\n",
    "                print(f\"    Primer ID: {primer_id}\")\n",
    "                for position, bases in positions.items():\n",
    "                    print(f\"      Position {position}:\")\n",
    "                    for base, percentage in bases.items():\n",
    "                        print(f\"        Base {base}: {percentage:.2f}%\")\n",
    "\n",
    "def sort_mismatch_frequencies_by_position(mismatch_frequencies):\n",
    "    \"\"\"\n",
    "    Sort the positions within each primer by position.\n",
    "    \"\"\"\n",
    "    sorted_frequencies = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: OrderedDict())))\n",
    "\n",
    "    for subtype, assays in mismatch_frequencies.items():\n",
    "        for assay, primers in assays.items():\n",
    "            for primer_id, positions in primers.items():\n",
    "                # Sort the positions by their integer key (position)\n",
    "                sorted_positions = OrderedDict(sorted(positions.items()))\n",
    "                sorted_frequencies[subtype][assay][primer_id] = sorted_positions\n",
    "\n",
    "    return sorted_frequencies\n",
    "\n",
    "def process_and_save_mismatch_percentages(json_file, output_file):\n",
    "    save_percentages_to_json(\n",
    "        calculate_percentages(\n",
    "            sort_mismatch_frequencies_by_position(\n",
    "                process_mismatches(json_file)\n",
    "            )\n",
    "        ), \n",
    "        output_file\n",
    "    )\n",
    "\n",
    "    # Optionally, you can include the print statements here:\n",
    "    sorted_mismatch_frequencies = sort_mismatch_frequencies_by_position(process_mismatches(json_file))\n",
    "    # print(sorted_mismatch_frequencies)\n",
    "    \n",
    "    percentages = calculate_percentages(sorted_mismatch_frequencies)\n",
    "    # print_percentages(percentages)\n",
    "    \n",
    "    print(f\"Percentages saved to {output_file}\")\n",
    "\n",
    "####### WRITE CODE\n",
    "for case in test_cases:\n",
    "    output_name_base = case[\"output_name_base\"]\n",
    "    assays = case[\"assays\"]\n",
    "    alignments = case[\"alignments\"]\n",
    "    rosalind_threshold = case[\"rosalind_threshold\"]\n",
    "    \n",
    "    # Print the variables to verify\n",
    "    print(f\"Output Name Base: {output_name_base}\")\n",
    "    print(f\"Assays: {assays}\")\n",
    "    print(f\"Alignments: {alignments}\")\n",
    "    print(f\"Rosalind Threshold: {rosalind_threshold}\")\n",
    "    print(\"---\")\n",
    "    \n",
    "    # Read in the Subtypes.csv file\n",
    "    subtype_data = pd.read_csv(alignments)\n",
    "\n",
    "    # File Naming\n",
    "    output_dir_base = \"Outputs/\" + output_name_base\n",
    "    output_aggregate = output_dir_base + \"_aggregate.json\"\n",
    "    output_organized = output_dir_base + \"_organized_data.json\"\n",
    "    output_organized_primer = output_dir_base + \"_organized_data_primer.json\"\n",
    "    output_coverage = output_dir_base + \"_coverage.json\"\n",
    "    output_csv = output_dir_base + \"_coverage.csv\"\n",
    "    output_agg_csv = output_dir_base + \"_coverage_agg.csv\"\n",
    "    output_case_info = output_dir_base + \"_case_info.json\"\n",
    "    output_by_base = output_dir_base + \"_by_base.json\"\n",
    "\n",
    "    write_run_info(output_case_info, output_name_base, assays, alignments, rosalind_threshold,\n",
    "               testing, globalAnalysis, primer_order_check, primer_proximity_check,\n",
    "               proximity_max_footprint, na_concentration, primer_concentration,\n",
    "               mg_concentration, dntp_concentration, corrected_mg_concentration,\n",
    "               expected_orders)\n",
    "\n",
    "    # This is for allowing a probe to not count in the 3' end but I haven't added it (For now - add NNNNN to the end of a probe)\n",
    "    PCR_primers = True\n",
    "\n",
    "    print(\"Analyzing assay: \" + assays)\n",
    "    print(\"Analyzing subtypes: \" + alignments)\n",
    "    # RUNNING THE CODE BELOW FOR ANALYSIS\n",
    "    all_mismatch_details = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for subtype in subtype_data[\"subtypes\"]: # Iterate over subtypes. This allows for analysis of multiple subtypes\n",
    "        print(f\"Processing subtype {subtype}\")\n",
    "        # Clean the subtype name to remove file extensions and paths\n",
    "        subtype_name = re.sub(r\"\\.fasta$\", \"\", subtype)\n",
    "        subtype_name = re.sub(r\".*/\", \"\", re.sub(r\"\\.fasta$\", \"\", subtype))\n",
    "        output_json = output_dir_base + \"_\" + subtype_name\n",
    "        # Read the sequences from the alignment file\n",
    "        seqfile = f\"Alignments/{subtype}\"\n",
    "        seqstring = read_fasta(seqfile)\n",
    "        #print(seqstring)\n",
    "        \n",
    "        numseqs = len(seqstring)\n",
    "        targets = pd.read_csv(assays) # Read the target assays\n",
    "        # Create a dictionary of primers\n",
    "        myprimers = {row[0]: read_fasta(f\"Assays/{row[0]}.fasta\") for _, row in targets.iterrows()}\n",
    "        #print(myprimers)\n",
    "        mismatch_details = []\n",
    "\n",
    "        total_sequences = len(seqstring) # Number of sequences in this Subtype\n",
    "        with tqdm.tqdm(total=total_sequences, desc=\"Processing sequences\", unit=\"iter\") as pbar:\n",
    "            for seq_record in seqstring: # Loop through each sequence in the subtype file\n",
    "                pbar.update(1)  # Update the progress bar for sequence iteration\n",
    "                for k, (assay_name, primers) in enumerate(myprimers.items()): # Loop through each assay and associated primers\n",
    "                    for primer in primers: # Loop through each primers\n",
    "                        seq = str(seq_record.seq)\n",
    "                        primer_seq = str(primer.seq)\n",
    "                        primer_length = len(primer_seq)\n",
    "                        primer_id = primer.id  # Assuming primer has an id attribute\n",
    "\n",
    "\n",
    "                        # Find best match and mismatches\n",
    "                        best_position, mismatches, alignment_type, num_mismatches = find_best_match(seq, primer_seq)\n",
    "                        mismatch_details.append((output_name_base, subtype, assay_name, seq_record.id, primer.id, primer_length, best_position, mismatches, alignment_type, num_mismatches, primer_seq))\n",
    "                #print(mismatch_details)\n",
    "                # Add mismatch details to the global list\n",
    "                all_mismatch_details.extend(mismatch_details)\n",
    "        \n",
    "            # Organize data for analysis\n",
    "            organized_data = organize_hierarchically(mismatch_details)\n",
    "            # Assess perfect coverage and coverage with single mismatch\n",
    "            organized_data = perfect_coverage(organized_data)\n",
    "        \n",
    "            # Assess coverage allowing mismatches except with n from end\n",
    "            organized_data = coverage_with_one_mismatch_n_bases_from_end(organized_data, 3)\n",
    "            organized_data = coverage_with_one_mismatch_n_bases_from_end(organized_data, 5)\n",
    "            #print(json.dumps(organized_data, indent=4))\n",
    "\n",
    "            # Assess coverage with one mismatch per assay\n",
    "            organized_data = coverage_with_one_mismatch_per_assay(organized_data)\n",
    "            #print(json.dumps(organized_data, indent=4))\n",
    "            \n",
    "            # Calculate ROSALIND assay score based on total number of mismatches per assay\n",
    "            organized_data = calculate_assay_score(organized_data)\n",
    "\n",
    "            # Add a check for primer proximity and order\n",
    "            organized_data = check_primer_proximity_and_order(organized_data)\n",
    "            #print(json.dumps(organized_data, indent=4))\n",
    "            update_json_file(output_organized, organized_data)\n",
    "        \n",
    "            # Aggregate ROSALIND scores\n",
    "            aggregated_data = aggregate_coverage(organized_data)\n",
    "            #print(json.dumps(aggregated_data, indent=4))\n",
    "            update_json_file(output_aggregate, aggregated_data)\n",
    "\n",
    "\n",
    "    # Create the final nested dictionary with a ROSALIND threshold\n",
    "    print(output_aggregate)\n",
    "    final_dict = assay_coverage_analysis(output_aggregate, rosalind_threshold)\n",
    "\n",
    "    # Print the final dictionary\n",
    "    print(json.dumps(final_dict, indent=4))\n",
    "    # Write the updated data back to the file\n",
    "    with open(output_coverage, 'w') as file:\n",
    "        json.dump(final_dict, file, indent=4)\n",
    "\n",
    "    create_csv_from_final_dict(final_dict, output_csv)\n",
    "    print(\"DONE\")\n",
    "    print(\"Total time: \" + str(time.time() - start_time))\n",
    "    \n",
    "    testing = case[\"testing\"]\n",
    "\n",
    "    global_diversity_csv = case.get(\"global_diversity\", \"Global_Diversity.csv\")\n",
    "\n",
    "    # if globalAnalysis: \n",
    "    #     global_diversity_csv = case[\"global_diversity\"]\n",
    "    # else:\n",
    "    #     global_diversity_csv = \"Global_Diversity.csv\"\n",
    "    if not testing:\n",
    "        calculate_weighted_aggregate(global_diversity_csv, output_csv, output_agg_csv)\n",
    "    \n",
    "    calculate_coverage_by_subtype_primer(output_organized, output_organized_primer, rosalind_threshold)\n",
    "\n",
    "    if singleBaseAnalysis:\n",
    "        process_and_save_mismatch_percentages(output_organized, output_by_base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
